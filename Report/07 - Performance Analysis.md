# Performance Analysis

In this document we are going to briefly outline the main results we managed to obtain from our models, a more general discussion of topic modelling as a subject and the challenges faces will be detailed in the conclusion part of the report.

Our original intention was to measure performance through the usage of measuring perplexity and coherence scores. However for some models this proved difficult and we also found through manual observation that we didn't think that our topic models performed particularly well, this is mostly due to the data that we are using, and we talk about this further in the conclusion. However we will try to make a brief discussion of what we gathered here now.

For the LDA model in 05 - LDA we got a coherence score of -15.72.
For the TFIDF model we got got a similar score, averaging -15.8.

Coherence is fundamentally a measure of often pairs of words appear together. We talk about this in the conclusion in slightly more detail, but due to the nature of SSH logs, there were many seperate instances of words appearing together often as SSH logs generally only have a certain number of pre-defined messages that they can output. This would suggest that we would have a high level of coherence. Essentially the model thinks that the words are related and "support" each other as they appear so often in close proximity to each other.

For perplexity, even though we did attempt to find a numerical value for it through the use of a function, we could pretty much infer the main result we can gain from it purely from our visual representation of the topic model. We found that generally with this Data a word would belong to one Topic with almost 100% certainty in the LDA section of the report, there was almost no words shared between topics. Therefore we can conclude that the Perplexity was low (which we would typically describe as "better") as words were not distributed across topics. However, in the TFIDF model we found that by looking out our visualisation for the topic model, that some of the words there seemed to have an overlap, which was surprising to find. Another potential observation is that while some topics dissapear when we hover a word, a few of the different numbered topics show up, but seems to have barely any percentage on the diagram, perhaps the model thinks that this word is associated with this topic, but just a miniscule amount.

There does seem to some slight correlation within a few topics in the TFIDF model output (found in 00-visualisations/tfidf_display.html) but even these links are quite superficial, and we can't clearly classify the topics from the words contained in them.

In the 00-Visualisations folder there is a file representing our topics called old_lda_10.html (to open these visualisations you will need to download the html file and open them in browser as GitHub does not preview html properly). This was our initial output from the LDA model, and had the same problems that we have mentioned above. In order to potentially improve this model we decided to try and improve the level of our data cleaning; updating and re-running that script and outputting new corpus files to pickles. However upon running with this new corpus we instead got a rather strange result, which can be seein in lda_display_10_topics, but also in lda_display (which shows an 8 topic test run). These topics have little overlap and seem to be evenly distributed apart, we are not quite not sure what this means, but seems to have some significance (albeit probably in a negative way). We thought this bizarre as in our research we have never seen this kind of pattern appear in any example. Strangely enough the TFIDF model remained of a consistent format for both versions of the corpus, looking similar to our original plot for the LDA model.

Topic 1 in the lda_display_10_topics file seems to include the largest variety of words (accounting for it having the largest size of all the topics) but there does not seem to be a clear topic trend in this or any of the other topics. This once again suggests that something has gone majorly wrong, probably again due to the data we used.

If we had had more time we may have tried to tune the parameters of the model more, and tried different numbers of topics, however the computations were both computationally taxing and time consuming, and we believe that the problem lies mostly in the original data, which no amount of tuning would fix.

In the tSNE section of the report we tried an alternate form of LDA modelling using dimensionality reduction, with the purpose to produce a cluster plot and draw some visual conclusions. However, once again, our model seems to have failed and the cluster plot which you can view in file clustering (available as a plot in html, but as this takes a while to load also available as a png screenshot of said plot) shows no obvious clusters. In fact the data seems to have been allocated all over the place. This could have been a problem with the model, but again we draw the conclusion it's probably to do with the data that we started with.

Overall it seems that our models think they performed well, but upon observation we cannot glean any useful information from our models and therefore do not think that they have run well, although in comparison it feels as if the TFIDF Topic Modelling has a slightly better output.