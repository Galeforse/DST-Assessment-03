{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative LDA analysis\n",
    "\n",
    "In this document we are trying to implement a different form of LDA in order to hopefully generate a different type of Topic Model visual representation. As it follows we will find that we could not draw any positive results from this implementation however we do have some discussion of the method and output in our final performance analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first start by importing the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bokeh.plotting import figure, show\n",
    "from bokeh.models import Label\n",
    "from urllib.request import urlopen\n",
    "from urllib.error import HTTPError\n",
    "import pickle\n",
    "import requests\n",
    "import datetime as dt\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "from nltk.corpus import wordnet\n",
    "import numpy as np\n",
    "import gzip\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "import pyLDAvis.sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading in data\n",
    "\n",
    "In this version of LDA, to visual the data how we would like we needed to switch packages from Gensim to sklearn's LatentDirichletAllocation package. This meant we have to do some preprocess of the data now instead of simply loading the pickle files as before. Hence we read in our data from Github and perform regex to remove punctuation, symbols, etc in our data so we can tokenise our data and effectively make our corpus again. We then aimed to before some dimensionality reduction to see how this would effect our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 log  index\n",
      "0  Jan  5 03:23:54 161.166.1.23 sshd[27076]: Fail...      0\n",
      "1  Jan  5 03:24:25 161.166.1.23 sshd[27087]: Disc...      1\n",
      "2  Jan  5 03:24:27 161.166.1.23 sshd[27090]: pam_...      2\n",
      "3  Jan  5 04:08:19 161.166.1.23 sshd[27584]: PAM ...      3\n",
      "4  Jan  5 04:08:21 161.166.1.23 sshd[27590]: pam_...      4\n",
      "1588052\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"https://github.com/Galeforse/DST-Assessment-03/raw/master/Data/master_log_10.csv.gz\")\n",
    "data_text=df[['log']]\n",
    "data_text['index']=data_text.index\n",
    "print(data_text[:5])\n",
    "print(len(data_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def regex(text):\n",
    "    text = re.sub(r\"[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+\",\" \",text) #Filters out ip adresses\n",
    "    text = re.sub(r\"[0-9]{2}\\:[0-9]{2}\\:[0-9]{2}\",\" \",text)#filters out times\n",
    "    text = re.sub(r'\\d+',\" \",text) #filters out numbers\n",
    "    text = re.sub(r\"[^A-Za-z0-9 ]+\",\" \",text) # filters out punctuation\n",
    "    text = re.sub(r\"XXXXX\",\" \",text) #filters out anonymised user\n",
    "    text = re.sub(r\"HHHHH\",\" \",text)\n",
    "    text = re.sub(r\"sshd\",\" \",text) #sshd comes up in every log and is not required\n",
    "    text = re.sub(r\"ruser\",\"user\",text)\n",
    "    text = re.sub(r\"rhost\",\"host\",text)\n",
    "    text = re.sub(r\"euid\",\"user\",text)\n",
    "    text = text.lower()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Jan  5 03:23:54 161.166.1.23 sshd[27076]: Fail...\n",
       "1    Jan  5 03:24:25 161.166.1.23 sshd[27087]: Disc...\n",
       "2    Jan  5 03:24:27 161.166.1.23 sshd[27090]: pam_...\n",
       "3    Jan  5 04:08:19 161.166.1.23 sshd[27584]: PAM ...\n",
       "4    Jan  5 04:08:21 161.166.1.23 sshd[27590]: pam_...\n",
       "Name: log, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = data_text['log']\n",
    "doc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "process = doc.map(regex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    jan             failed password for   from   p...\n",
       "1    jan             disconnecting  too many authen...\n",
       "2    jan             pam unix   auth  authenticatio...\n",
       "3    jan             pam service    ignoring max re...\n",
       "4    jan             pam unix   auth  authenticatio...\n",
       "Name: log, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use CountVectorizer to count the number of times a word occurs in our corpus. We then want to learn the dictionary and return a document-term matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_count_vectorizer = CountVectorizer(stop_words='english', max_features=40000)\n",
    "data_document_term_matrix = data_count_vectorizer.fit_transform(process)\n",
    "n_topics=10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know have all we know to train our LDA model again using the sklearn LDA package this time and then fit this to a topic matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to read model from pickle\n",
      "Pickle not found, creating model and saving to pickle\n",
      "Pickle saved. Time taken: 0:00:00.001003\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print(\"Attempting to read model from pickle\")\n",
    "    fp=gzip.open(urlopen('https://github.com/Galeforse/DST-Assessment-03/raw/master/Data/main/lda_model2.pkl.gz'),'rb')\n",
    "    lda_model2=pickle.load(fp)\n",
    "    fp.close()\n",
    "    print(\"Model read from pickle.\")\n",
    "except HTTPError as err:\n",
    "    if err.code == 404:\n",
    "        print(\"Pickle not found, creating model and saving to pickle\")\n",
    "        start=dt.datetime.now()\n",
    "        lda_model2 = LatentDirichletAllocation(n_components=n_topics, learning_method='online', random_state=0,verbose=0)\n",
    "        fp=gzip.open('G:/Users/Gabriel/Documents/Education/UoB/GitHubDesktop/DST-Assessment-03/data/main/lda_model2.pkl.gz','wb')\n",
    "        pickle.dump(lda_model2,fp)\n",
    "        fp.close()\n",
    "        print(\"Pickle saved. Time taken: \" + str(dt.datetime.now()-start))\n",
    "    else:\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to read topic matrix from pickle\n",
      "Pickle not found, creating topic matrix and saving to pickle\n",
      "Pickle saved. Time taken: 0:40:27.025222\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print(\"Attempting to read topic matrix from pickle\")\n",
    "    fp=gzip.open(urlopen('https://github.com/Galeforse/DST-Assessment-03/raw/master/Data/main/lda_topic_matrix.pkl.gz'),'rb')\n",
    "    lda_topic_matrix=pickle.load(fp)\n",
    "    fp.close()\n",
    "    print(\"Topic matrix read from pickle.\")\n",
    "except HTTPError as err:\n",
    "    if err.code == 404:\n",
    "        print(\"Pickle not found, creating topic matrix and saving to pickle\")\n",
    "        start=dt.datetime.now()\n",
    "        lda_topic_matrix = lda_model2.fit_transform(data_document_term_matrix)\n",
    "        fp=gzip.open('G:/Users/Gabriel/Documents/Education/UoB/GitHubDesktop/DST-Assessment-03/data/main/lda_topic_matrix.pkl.gz','wb')\n",
    "        pickle.dump(lda_topic_matrix,fp)\n",
    "        fp.close()\n",
    "        print(\"Pickle saved. Time taken: \" + str(dt.datetime.now()-start))\n",
    "    else:\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define helper functions\n",
    "def get_keys(topic_matrix):\n",
    "    '''\n",
    "    returns an integer list of predicted topic \n",
    "    categories for a given topic matrix\n",
    "    '''\n",
    "    keys = topic_matrix.argmax(axis=1).tolist()\n",
    "    return keys\n",
    "\n",
    "def keys_to_counts(keys):\n",
    "    '''\n",
    "    returns a tuple of topic categories and their \n",
    "    accompanying magnitudes for a given list of keys\n",
    "    '''\n",
    "    count_pairs = Counter(keys).items()\n",
    "    categories = [pair[0] for pair in count_pairs]\n",
    "    counts = [pair[1] for pair in count_pairs]\n",
    "    return (categories, counts)\n",
    "\n",
    "lda_keys = get_keys(lda_topic_matrix)\n",
    "lda_categories, lda_counts = keys_to_counts(lda_keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we decided to do some dimensionality reduction use t-Distributed Stochastic Neighbour Embedding (t-SNE) which minimises the divergence between two distributions. t-SNE maps the multi-dimensional data to a lower dimensional space and analyses any observed clusters based off the similarity of data points which feature multiple times. After this however, no inferences can be made as the input features cannot be identified anymore, hence why we are using it to mainly to try and visualise our data, allowing us to hopefully draw some conclusions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print(\"Attempting to read tsne model from pickle\")\n",
    "    fp=gzip.open(urlopen('https://github.com/Galeforse/DST-Assessment-03/raw/master/Data/main/tsne_model.pkl.gz'),'rb')\n",
    "    tsne_lda_model2=pickle.load(fp)\n",
    "    fp.close()\n",
    "    print(\"Model read from pickle.\")\n",
    "except HTTPError as err:\n",
    "    if err.code == 404:\n",
    "        print(\"Pickle not found, creating tsne model and saving to pickle\")\n",
    "        start=dt.datetime.now()\n",
    "        tsne_lda_model2 = TSNE(n_components=2, perplexity=50, learning_rate=100, \n",
    "                        n_iter=2000, verbose=1, random_state=0, angle=0.75)\n",
    "        fp=gzip.open('G:/Users/Gabriel/Documents/Education/UoB/GitHubDesktop/DST-Assessment-03/data/main/tsne_model.pkl.gz','wb')\n",
    "        pickle.dump(tsne_lda_model2,fp)\n",
    "        fp.close()\n",
    "        print(\"Pickle saved. Time taken: \" + str(dt.datetime.now()-start))\n",
    "    else:\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print(\"Attempting to read tsne vectors from pickle\")\n",
    "    fp=gzip.open(urlopen('https://github.com/Galeforse/DST-Assessment-03/raw/master/Data/main/tsne_lda_vectors.pkl.gz'),'rb')\n",
    "    tsne_lda_vectors=pickle.load(fp)\n",
    "    fp.close()\n",
    "    print(\"Model read from pickle.\")\n",
    "except HTTPError as err:\n",
    "    if err.code == 404:\n",
    "        print(\"Pickle not found, creating tsne vectors and saving to pickle\")\n",
    "        start=dt.datetime.now()\n",
    "        tsne_lda_vectors = tsne_lda_model2.fit_transform(lda_topic_matrix)\n",
    "        fp=gzip.open('G:/Users/Gabriel/Documents/Education/UoB/GitHubDesktop/DST-Assessment-03/data/main/tsne_lda_vectors.pkl.gz','wb')\n",
    "        pickle.dump(tsne_lda_vectors,fp)\n",
    "        fp.close()\n",
    "        print(\"Pickle saved. Time taken: \" + str(dt.datetime.now()-start))\n",
    "    else:\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "colormap = np.array([\n",
    "    \"#1f77b4\", \"#aec7e8\", \"#ff7f0e\", \"#ffbb78\", \"#2ca02c\",\n",
    "    \"#98df8a\", \"#d62728\", \"#ff9896\", \"#9467bd\", \"#c5b0d5\",\n",
    "    \"#8c564b\", \"#c49c94\", \"#e377c2\", \"#f7b6d2\", \"#7f7f7f\",\n",
    "    \"#c7c7c7\", \"#bcbd22\", \"#dbdb8d\", \"#17becf\", \"#9edae5\" ])\n",
    "colormap = colormap[:n_topics]\n",
    "\n",
    "plot = figure(title=\"t-SNE Clustering of {} LDA Topics\".format(n_topics), plot_width=700, plot_height=700)\n",
    "plot.scatter(x=tsne_lda_vectors[:,0], y=tsne_lda_vectors[:,1], color=colormap[lda_keys])\n",
    "show(plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wanted to implement the perplexity and coherence again however ran into troubles in doing so and eventually ran out of time after many failed attempts at doing so. From the conclusions we draw about the scatter plot above we also were not sure whether it was worth evaluating these values; this will be talked about further in the performance analysis document, coming up next."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
