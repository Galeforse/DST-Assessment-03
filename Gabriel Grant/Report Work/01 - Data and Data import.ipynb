{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 - Data and Data import\n",
    "\n",
    "In this first section we will detail everything about the dataset and the pre-preparation we had to do to it in order to allow us to use it for our project. The [data](https://www.simpleweb.org/wiki/index.php/SSH_datasets) that we used was a selection of SSH logs from the linked web page. Originally we wanted to attempt to access the data directly from the [source link](http://traces.simpleweb.org/ssh_datasets/dataset2_log_files.tgz), however this proved problematic due to the nature of the file.\n",
    "\n",
    "We downloaded and inspected the data to find that it was off the following structure:\n",
    "\n",
    "* `dataset2_log_files.tgz`\n",
    "    * `dataset2_log_files.tar`\n",
    "        * `ip-address1`\n",
    "            * `auth.log.anon`\n",
    "        * `ip-address2`\n",
    "            * `auth.log.anon`\n",
    "        *      ...\n",
    "        * `ip-address73`\n",
    "            * `auth.log.anon`\n",
    "            \n",
    "Essentially what we had were 73 seperate `.log` files for different ips on the network that the data was recorded on (Campus network of the University of Twente, Netherlands). In order to work with this in the way that we wanted we had to do rather a lot of manual file management to allow us to use this data.\n",
    "\n",
    "As part of this, we read the details on the page that we downloaded the data from, which had this to say:\n",
    ">     1. Merging: On some machines, the authentication logs were distributed over hostname.messages and hostname.warn. We have merged those log files, sorted them again (if necessary), and removed any introduced duplicates.\n",
    "    2. Renaming: The file names have been changed from hostname.extension into anonymized_IP_address.extension. As such, the log files can easily be correlated with the flow data.\n",
    "    3. Anonymization: We have replaced any usernames by \"XXXXX\" and hostnames by the anonymized IP address of the considered host.\n",
    "    \n",
    "Due to the anonymising of the data the ip address we see are not the originals, but every instance of them corresponds to the same value. Another key point that we will touch on in the next section of the report is how the usernames have been anonymised to \"XXXXX\". This anonymising process has caused the log files to be represented as `auth.log.anon` and so for ease of reading and accessibility while keeping potentially important data intact we had to do the following steps:\n",
    "* Copy each `auth.log.anon` file in turn to the GitHub repository on my hardrive in order to push to the master.\n",
    "    * As part of this process as we copied each in turn from their respective ip-address folder we renamed the file to \"1.log\" for the first file, \"2.log\" for the second file etc.\n",
    "    * We've changed the file extension and this allows us to read the log files in our text editor or a program such as excel as normal.\n",
    "* \"26.log\" was especially large and this highlighted that it would be beneficial to compress with gzip our log files, so we did this for all 73 files.\n",
    "* In order to keep track of the ip addresses so that we know where each log has come from, we made a text file, seperated by commas, detailing each ip address in turn (so the 1st ip address corresponds to it's appropriate .log file.\n",
    "\n",
    "In the code that follows you'll see how we then handled this data in order to create a good dataset to work on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First we start by importing the packages we require for this document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import tarfile\n",
    "import zipfile\n",
    "from urllib.request import urlopen\n",
    "from io import StringIO\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this next step we open our list of ips mentioned above read them into the variable `ip`. We check the length and see that the list is 73 items long (as expected) and that the first ips are correct when cross referenced with our original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['161.166.1.22',\n",
       " '161.166.1.23',\n",
       " '161.166.1.27',\n",
       " '161.166.1.38',\n",
       " '161.166.1.54']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global ip\n",
    "text_file = urlopen(\"https://github.com/Galeforse/DST-Assessment-03/raw/master/Data/anon_ips_list2.txt\")\n",
    "ip = text_file.read().decode('utf-8').split(',')\n",
    "print(len(ip))\n",
    "ip[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to stream line the import process we defined the following function which does several things in one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_import(x):\n",
    "    global ip\n",
    "    y = str(x)\n",
    "    root = \"https://github.com/Galeforse/DST-Assessment-03/raw/master/Data/log_files/\"\n",
    "    df1 = pd.read_csv(root + y +\".log.gz\",names=[\"log\"])   \n",
    "    df1.insert(0, \"anon_ip\",ip[x-1], allow_duplicates = True)\n",
    "    return(df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function takes one variable as an input and this corresponds to the log number that we wish to import. Then using string manipulation we read in the log file with \n",
    "\n",
    "```pd.read_csv(root + y + \".log.gz\",names=[\"log\"]```\n",
    "\n",
    "This function combines the root of the url (the folder the file is found in) the number of the log file (gathered from the function's one variable) and then adds the file extension. It then adds a column detailing the corresponding anonymised ip from the list of ips we imported in the previous step.\n",
    "\n",
    "Later on in this report we became more familiar with using RegEx in python and we may have been able to streamline this entire workbook to a much smaller file by matching files and combining in a single function, but at the time we hadn't quite got to grips with using regular expressions, and the above function worked well for what we needed to do.\n",
    "\n",
    "We quickly test the function on our first log file, noticing that the `anon_ip` is the same for all values as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>anon_ip</th>\n",
       "      <th>log</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>161.166.1.22</td>\n",
       "      <td>Jan  5 08:13:51 161.166.1.22 sshd[42590]: pam_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>161.166.1.22</td>\n",
       "      <td>Jan  5 08:13:53 161.166.1.22 sshd[42590]: Fail...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>161.166.1.22</td>\n",
       "      <td>Jan  5 08:13:55 161.166.1.22 sshd[42590]: Fail...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>161.166.1.22</td>\n",
       "      <td>Jan  5 08:13:57 161.166.1.22 sshd[42590]: Fail...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>161.166.1.22</td>\n",
       "      <td>Jan  5 08:14:00 161.166.1.22 sshd[42590]: Fail...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113063</th>\n",
       "      <td>161.166.1.22</td>\n",
       "      <td>Feb 28 23:01:15 161.166.1.22 sshd[52192]: Rece...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113064</th>\n",
       "      <td>161.166.1.22</td>\n",
       "      <td>Feb 28 23:01:15 161.166.1.22 sshd[52192]: pam_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113065</th>\n",
       "      <td>161.166.1.22</td>\n",
       "      <td>Feb 28 23:01:15 161.166.1.22 sshd[52192]: pam_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113066</th>\n",
       "      <td>161.166.1.22</td>\n",
       "      <td>Feb 28 23:01:15 161.166.1.22 sshd[52218]: Acce...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113067</th>\n",
       "      <td>161.166.1.22</td>\n",
       "      <td>Feb 28 23:01:15 161.166.1.22 sshd[52218]: pam_...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>113068 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             anon_ip                                                log\n",
       "0       161.166.1.22  Jan  5 08:13:51 161.166.1.22 sshd[42590]: pam_...\n",
       "1       161.166.1.22  Jan  5 08:13:53 161.166.1.22 sshd[42590]: Fail...\n",
       "2       161.166.1.22  Jan  5 08:13:55 161.166.1.22 sshd[42590]: Fail...\n",
       "3       161.166.1.22  Jan  5 08:13:57 161.166.1.22 sshd[42590]: Fail...\n",
       "4       161.166.1.22  Jan  5 08:14:00 161.166.1.22 sshd[42590]: Fail...\n",
       "...              ...                                                ...\n",
       "113063  161.166.1.22  Feb 28 23:01:15 161.166.1.22 sshd[52192]: Rece...\n",
       "113064  161.166.1.22  Feb 28 23:01:15 161.166.1.22 sshd[52192]: pam_...\n",
       "113065  161.166.1.22  Feb 28 23:01:15 161.166.1.22 sshd[52192]: pam_...\n",
       "113066  161.166.1.22  Feb 28 23:01:15 161.166.1.22 sshd[52218]: Acce...\n",
       "113067  161.166.1.22  Feb 28 23:01:15 161.166.1.22 sshd[52218]: pam_...\n",
       "\n",
       "[113068 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_import(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Log Files\n",
    "\n",
    "We couldn't find a way to automate the next steps any better and so imported each of the 73 log files to it's own dataframe in turn. (conducted in blocks of 10 in case of errors arising)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported data in: 0:00:08.681687\n"
     ]
    }
   ],
   "source": [
    "start = dt.datetime.now()\n",
    "df1 = log_import(1)\n",
    "df2 = log_import(2)\n",
    "df3 = log_import(3)\n",
    "df4 = log_import(4)\n",
    "df5 = log_import(5)\n",
    "df6 = log_import(6)\n",
    "df7 = log_import(7)\n",
    "df8 = log_import(8)\n",
    "df9 = log_import(9)\n",
    "print(\"Imported data in: \"+str(dt.datetime.now()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported data in: 0:00:09.559229\n"
     ]
    }
   ],
   "source": [
    "start = dt.datetime.now()\n",
    "df10 = log_import(10)\n",
    "df11 = log_import(11)\n",
    "df12 = log_import(12)\n",
    "df13 = log_import(13)\n",
    "df14 = log_import(14)\n",
    "df15 = log_import(15)\n",
    "df16 = log_import(16)\n",
    "df17 = log_import(17)\n",
    "df18 = log_import(18)\n",
    "df19 = log_import(19)\n",
    "print(\"Imported data in: \"+str(dt.datetime.now()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported data in : 0:00:26.687143\n"
     ]
    }
   ],
   "source": [
    "start = dt.datetime.now()\n",
    "df20 = log_import(20)\n",
    "df21 = log_import(21)\n",
    "df22 = log_import(22)\n",
    "df23 = log_import(23)\n",
    "df24 = log_import(24)\n",
    "df25 = log_import(25)\n",
    "df26 = log_import(26)\n",
    "df27 = log_import(27)\n",
    "df28 = log_import(28)\n",
    "df29 = log_import(29)\n",
    "print(\"Imported data in : \"+str(dt.datetime.now()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported data in: 0:00:06.506241\n"
     ]
    }
   ],
   "source": [
    "start = dt.datetime.now()\n",
    "df30 = log_import(30)\n",
    "df31 = log_import(31)\n",
    "df32 = log_import(32)\n",
    "df33 = log_import(33)\n",
    "df34 = log_import(34)\n",
    "df35 = log_import(35)\n",
    "df36 = log_import(36)\n",
    "df37 = log_import(37)\n",
    "df38 = log_import(38)\n",
    "df39 = log_import(39)\n",
    "print(\"Imported data in: \"+str(dt.datetime.now()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported data in: 0:00:09.440936\n"
     ]
    }
   ],
   "source": [
    "start = dt.datetime.now()\n",
    "df40 = log_import(40)\n",
    "df41 = log_import(41)\n",
    "df42 = log_import(42)\n",
    "df43 = log_import(43)\n",
    "df44 = log_import(44)\n",
    "df45 = log_import(45)\n",
    "df46 = log_import(46)\n",
    "df47 = log_import(47)\n",
    "df48 = log_import(48)\n",
    "df49 = log_import(49)\n",
    "print(\"Imported data in: \"+str(dt.datetime.now()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported data in: 0:00:11.738392\n"
     ]
    }
   ],
   "source": [
    "start = dt.datetime.now()\n",
    "df50 = log_import(50)\n",
    "df51 = log_import(51)\n",
    "df52 = log_import(52)\n",
    "df53 = log_import(53)\n",
    "df54 = log_import(54)\n",
    "df55 = log_import(55)\n",
    "df56 = log_import(56)\n",
    "df57 = log_import(57)\n",
    "df58 = log_import(58)\n",
    "df59 = log_import(59)\n",
    "print(\"Imported data in: \"+str(dt.datetime.now()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported data in: 0:00:14.427739\n"
     ]
    }
   ],
   "source": [
    "start = dt.datetime.now()\n",
    "df60 = log_import(60)\n",
    "df61 = log_import(61)\n",
    "df62 = log_import(62)\n",
    "df63 = log_import(63)\n",
    "df64 = log_import(64)\n",
    "df65 = log_import(65)\n",
    "df66 = log_import(66)\n",
    "df67 = log_import(67)\n",
    "df68 = log_import(68)\n",
    "df69 = log_import(69)\n",
    "df70 = log_import(70)\n",
    "df71 = log_import(71)\n",
    "df72 = log_import(72)\n",
    "df73 = log_import(73)\n",
    "print(\"Imported data in: \"+str(dt.datetime.now()-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our penultimate step in setting up this data we use the pandas `concat` function to combine all of our log files into one, one after another. We then reset the index of the file and drop the old indexs (which become a new column when the index is reset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftest = pd.concat([df2,df3,df4,df5,df6,df7,df8,df9,df10,df11,df12,df13,df14,df15,df16,df17,df18,df19,df20,df21,df22,df23,df24,df25,df26,df27,df28,df29,df30,\n",
    "                       df31,df32,df33,df34,df35,df36,df37,df38,df39,df40,df41,df42,df43,df44,df45,df46,df47,df48,df49,df50,df51,df52,df53,df54,df55,df56,df57,df58,\n",
    "                       df59,df60,df61,df62,df63,df64,df65,df66,df67,df68,df69,df70,df71,df72,df73])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>anon_ip</th>\n",
       "      <th>log</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>161.166.1.23</td>\n",
       "      <td>Jan  5 03:23:40 161.166.1.23 sshd[27076]: pam_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>161.166.1.23</td>\n",
       "      <td>Jan  5 03:23:42 161.166.1.23 sshd[27076]: Fail...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>161.166.1.23</td>\n",
       "      <td>Jan  5 03:23:44 161.166.1.23 sshd[27076]: Fail...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>161.166.1.23</td>\n",
       "      <td>Jan  5 03:23:47 161.166.1.23 sshd[27076]: Fail...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>161.166.1.23</td>\n",
       "      <td>Jan  5 03:23:49 161.166.1.23 sshd[27076]: Fail...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15880517</th>\n",
       "      <td>161.166.236.154</td>\n",
       "      <td>Feb 23 05:20:38 161.166.236.154 sshd[16644]: I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15880518</th>\n",
       "      <td>161.166.236.154</td>\n",
       "      <td>Feb 23 05:20:41 161.166.236.154 sshd[16662]: I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15880519</th>\n",
       "      <td>161.166.236.154</td>\n",
       "      <td>Feb 23 05:20:44 161.166.236.154 sshd[16708]: I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15880520</th>\n",
       "      <td>161.166.236.154</td>\n",
       "      <td>Feb 23 06:29:04 161.166.236.154 sshd[1282]: Us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15880521</th>\n",
       "      <td>161.166.236.154</td>\n",
       "      <td>Feb 23 06:30:00 161.166.236.154 sshd[1464]: Us...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15880522 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  anon_ip                                                log\n",
       "0            161.166.1.23  Jan  5 03:23:40 161.166.1.23 sshd[27076]: pam_...\n",
       "1            161.166.1.23  Jan  5 03:23:42 161.166.1.23 sshd[27076]: Fail...\n",
       "2            161.166.1.23  Jan  5 03:23:44 161.166.1.23 sshd[27076]: Fail...\n",
       "3            161.166.1.23  Jan  5 03:23:47 161.166.1.23 sshd[27076]: Fail...\n",
       "4            161.166.1.23  Jan  5 03:23:49 161.166.1.23 sshd[27076]: Fail...\n",
       "...                   ...                                                ...\n",
       "15880517  161.166.236.154  Feb 23 05:20:38 161.166.236.154 sshd[16644]: I...\n",
       "15880518  161.166.236.154  Feb 23 05:20:41 161.166.236.154 sshd[16662]: I...\n",
       "15880519  161.166.236.154  Feb 23 05:20:44 161.166.236.154 sshd[16708]: I...\n",
       "15880520  161.166.236.154  Feb 23 06:29:04 161.166.236.154 sshd[1282]: Us...\n",
       "15880521  161.166.236.154  Feb 23 06:30:00 161.166.236.154 sshd[1464]: Us...\n",
       "\n",
       "[15880522 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dftest = dftest.reset_index()\n",
    "dftest = dftest.drop(columns=[\"index\"],axis=1)\n",
    "dftest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting and compressing for further use\n",
    "\n",
    "Finally we write our data set to a new log file compressed with .gzip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log written to czv.gz in: 0:02:02.223963\n"
     ]
    }
   ],
   "source": [
    "start = dt.datetime.now()\n",
    "dftest.to_csv(path_or_buf=\"G:/Users/Gabriel/Documents/Education/UoB/GitHubDesktop/DST-Assessment-03/Data/master_log.csv.gz\",index=False,compression=\"gzip\")\n",
    "print(\"Log written to czv.gz in: \"+str(dt.datetime.now()-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For testing purposes and to deal with memory limitations in some of the next methods in the report we also made several smaller versions of the data set using the sample function. We again reset the index and made sure that the data would appear in the same ip order as the original data set.\n",
    "### 10 percent data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "smp_size = round(len(dftest)/10)\n",
    "dftest_10 = dftest.sample(n=smp_size,random_state=7)\n",
    "dftest_10 = dftest_10.sort_index()\n",
    "dftest_10 = dftest_10.reset_index()\n",
    "dftest_10 = dftest_10.drop(columns=[\"index\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log written to czv.gz in: 0:00:13.629114\n"
     ]
    }
   ],
   "source": [
    "start = dt.datetime.now()\n",
    "dftest_10.to_csv(path_or_buf=\"G:/Users/Gabriel/Documents/Education/UoB/GitHubDesktop/DST-Assessment-03/Data/master_log_10.csv.gz\",index=False,compression=\"gzip\")\n",
    "print(\"Log written to czv.gz in: \"+str(dt.datetime.now()-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 percent data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "smp_size = round(len(dftest)/20)\n",
    "dftest_5 = dftest.sample(n=smp_size,random_state=7)\n",
    "dftest_5 = dftest_5.sort_index()\n",
    "dftest_5 = dftest_5.reset_index()\n",
    "dftest_5 = dftest_5.drop(columns=[\"index\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log written to czv.gz in: 0:00:07.067844\n"
     ]
    }
   ],
   "source": [
    "start = dt.datetime.now()\n",
    "dftest_5.to_csv(path_or_buf=\"G:/Users/Gabriel/Documents/Education/UoB/GitHubDesktop/DST-Assessment-03/Data/master_log_5.csv.gz\",index=False,compression=\"gzip\")\n",
    "print(\"Log written to czv.gz in: \"+str(dt.datetime.now()-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 50 percent data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "smp_size = round(len(dftest)/2)\n",
    "dftest_50 = dftest.sample(n=smp_size,random_state=7)\n",
    "dftest_50 = dftest_50.sort_index()\n",
    "dftest_50 = dftest_50.reset_index()\n",
    "dftest_50 = dftest_50.drop(columns=[\"index\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log written to czv.gz in: 0:01:04.750243\n"
     ]
    }
   ],
   "source": [
    "start = dt.datetime.now()\n",
    "dftest_50.to_csv(path_or_buf=\"G:/Users/Gabriel/Documents/Education/UoB/GitHubDesktop/DST-Assessment-03/Data/master_log_50.csv.gz\",index=False,compression=\"gzip\")\n",
    "print(\"Log written to czv.gz in: \"+str(dt.datetime.now()-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 40 percent data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "smp_size = round((len(dftest)*4)/10)\n",
    "dftest_40 = dftest.sample(n=smp_size,random_state=7)\n",
    "dftest_40 = dftest_40.sort_index()\n",
    "dftest_40 = dftest_40.reset_index()\n",
    "dftest_40 = dftest_40.drop(columns=[\"index\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log written to czv.gz in: 0:00:49.837420\n"
     ]
    }
   ],
   "source": [
    "start = dt.datetime.now()\n",
    "dftest_40.to_csv(path_or_buf=\"G:/Users/Gabriel/Documents/Education/UoB/GitHubDesktop/DST-Assessment-03/Data/master_log_40.csv.gz\",index=False,compression=\"gzip\")\n",
    "print(\"Log written to czv.gz in: \"+str(dt.datetime.now()-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now on GitHubDesktop we just save and push these files to the repository, and are now ready to access them from anywhere, for use in further code."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
