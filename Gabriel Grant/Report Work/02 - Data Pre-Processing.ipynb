{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-Processing.\n",
    "\n",
    "Now that we have sorted out our dataset we are going to pre-process the data in order to be used with our topic models. Data pre-processing and cleaning is extremely important for Topic modelling as it revolves around text. We must remove the unnecessary to help speed up our algorithm and to help it to run smoother. In this document we will use a variety of different tools available to us to clean our text.\n",
    "\n",
    "It is worth noting that due to the amount of data processed and the type of data we are processing (SSH Logs), in later documents of the report we will find that some of the \"words\" that are left in after this process are not particularly useful or suitable for topic modelling however without manual inspection (which has been conducted in this report for some of the more obvious words) it is difficult to remove these words and the later models do not have much to say about them. However we do successfully remove more obvious problematic values such as those featuring punctuation and numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by importing the packages we are going to use in this document as usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from urllib.request import urlopen\n",
    "from urllib.error import HTTPError\n",
    "import pickle\n",
    "import requests\n",
    "import datetime as dt\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "from nltk.corpus import wordnet\n",
    "import numpy as np\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to read in the data generated in the previous document. We have decided we are going to work on the 10% sampled data throughout this report. There are several reasons for this, in our testing we used the 5% data but feel that this may have been too small (though quick to load) and on the other hand when we tried to use larger datasets like the 50% data, when we tried to generate models we ran into a computational time problem. The model that was being generated itself wasn't too large for most computers to handle but was taking several hours to compute and we decided to use this smaller data set for a better balance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data fetched in: 0:00:05.653516\n"
     ]
    }
   ],
   "source": [
    "start = dt.datetime.now()\n",
    "df = pd.read_csv(\"https://github.com/Galeforse/DST-Assessment-03/raw/master/Data/master_log_10.csv.gz\")\n",
    "print(\"Data fetched in:\" ,dt.datetime.now()-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check the length of our dataset to make sure it is the size that we expect (10% of the original approx. 15,000,000 points) and have a look at the first few values in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1588052\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>anon_ip</th>\n",
       "      <th>log</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>161.166.1.23</td>\n",
       "      <td>Jan  5 03:23:54 161.166.1.23 sshd[27076]: Fail...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>161.166.1.23</td>\n",
       "      <td>Jan  5 03:24:25 161.166.1.23 sshd[27087]: Disc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>161.166.1.23</td>\n",
       "      <td>Jan  5 03:24:27 161.166.1.23 sshd[27090]: pam_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>161.166.1.23</td>\n",
       "      <td>Jan  5 04:08:19 161.166.1.23 sshd[27584]: PAM ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>161.166.1.23</td>\n",
       "      <td>Jan  5 04:08:21 161.166.1.23 sshd[27590]: pam_...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        anon_ip                                                log\n",
       "0  161.166.1.23  Jan  5 03:23:54 161.166.1.23 sshd[27076]: Fail...\n",
       "1  161.166.1.23  Jan  5 03:24:25 161.166.1.23 sshd[27087]: Disc...\n",
       "2  161.166.1.23  Jan  5 03:24:27 161.166.1.23 sshd[27090]: pam_...\n",
       "3  161.166.1.23  Jan  5 04:08:19 161.166.1.23 sshd[27584]: PAM ...\n",
       "4  161.166.1.23  Jan  5 04:08:21 161.166.1.23 sshd[27590]: pam_..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also check a random section of the data somewhere in the middle and can see that the order has been preserved and we have a mix of data from different ips, therefore we have a good sample of the overall data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>anon_ip</th>\n",
       "      <th>log</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30000</th>\n",
       "      <td>161.166.1.38</td>\n",
       "      <td>Jan 21 17:55:47 161.166.1.38 sshd[6843]: pam_u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30001</th>\n",
       "      <td>161.166.1.38</td>\n",
       "      <td>Jan 21 17:55:47 161.166.1.38 sshd[6843]: pam_l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30002</th>\n",
       "      <td>161.166.1.38</td>\n",
       "      <td>Jan 21 17:55:53 161.166.1.38 sshd[6845]: pam_u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30003</th>\n",
       "      <td>161.166.1.38</td>\n",
       "      <td>Jan 21 17:56:03 161.166.1.38 sshd[6849]: Inval...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30004</th>\n",
       "      <td>161.166.1.38</td>\n",
       "      <td>Jan 21 17:56:03 161.166.1.38 sshd[6849]: pam_l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30005</th>\n",
       "      <td>161.166.1.38</td>\n",
       "      <td>Jan 21 17:56:19 161.166.1.38 sshd[6855]: pam_l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30006</th>\n",
       "      <td>161.166.1.38</td>\n",
       "      <td>Jan 21 17:56:24 161.166.1.38 sshd[6857]: Inval...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30007</th>\n",
       "      <td>161.166.1.38</td>\n",
       "      <td>Jan 21 17:56:31 161.166.1.38 sshd[6859]: Faile...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            anon_ip                                                log\n",
       "30000  161.166.1.38  Jan 21 17:55:47 161.166.1.38 sshd[6843]: pam_u...\n",
       "30001  161.166.1.38  Jan 21 17:55:47 161.166.1.38 sshd[6843]: pam_l...\n",
       "30002  161.166.1.38  Jan 21 17:55:53 161.166.1.38 sshd[6845]: pam_u...\n",
       "30003  161.166.1.38  Jan 21 17:56:03 161.166.1.38 sshd[6849]: Inval...\n",
       "30004  161.166.1.38  Jan 21 17:56:03 161.166.1.38 sshd[6849]: pam_l...\n",
       "30005  161.166.1.38  Jan 21 17:56:19 161.166.1.38 sshd[6855]: pam_l...\n",
       "30006  161.166.1.38  Jan 21 17:56:24 161.166.1.38 sshd[6857]: Inval...\n",
       "30007  161.166.1.38  Jan 21 17:56:31 161.166.1.38 sshd[6859]: Faile..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[30000:30008]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Jan 21 22:25:29 161.166.9.144 sshd[6993]: pam_unix(sshd:session): session closed for user XXXXX'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"log\"].iloc[500000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1588052\n",
      "                                                 log  index\n",
      "0  Jan  5 03:23:54 161.166.1.23 sshd[27076]: Fail...      0\n",
      "1  Jan  5 03:24:25 161.166.1.23 sshd[27087]: Disc...      1\n",
      "2  Jan  5 03:24:27 161.166.1.23 sshd[27090]: pam_...      2\n",
      "3  Jan  5 04:08:19 161.166.1.23 sshd[27584]: PAM ...      3\n",
      "4  Jan  5 04:08:21 161.166.1.23 sshd[27590]: pam_...      4\n"
     ]
    }
   ],
   "source": [
    "data_text = df[['log']]\n",
    "data_text[\"index\"] = data_text.index\n",
    "print(len(data_text))\n",
    "print(data_text[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regex\n",
    "\n",
    "In this subsection we are going to make use of Python's in built regular expression functions to filter the text. This function allows us to search through strings to match various user defined patterns of letters, numbers, and/or special characters. \n",
    "\n",
    "Below we definte a function that we call regex, for which we specify certain regular expression substitutions (substituting with a blank space; in the long term this does not matter as when we lemmatize the data further down the document all the spaces in the string will be removed).\n",
    "\n",
    "These regular expression substitutions were defined from analysis of the log files, though as previously mentioned there are several nonsense words in the data that have been left in due to their sheer number. Most of these are either more technically words generated by the computer, or names of various companies/domains that are referenced in the log. We acknowledge that not successfully filtering out all of these junk words may have had a negative effect on the overall performance of the models later on.\n",
    "\n",
    "It is here that we filter out the anonymised usernames \"XXXXX\" that we mentioned in the start of the previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def regex(text):\n",
    "    text = re.sub(r\"[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+\",\" \",text) #Filters out ip adresses\n",
    "    text = re.sub(r\"[0-9]{2}\\:[0-9]{2}\\:[0-9]{2}\",\" \",text)#filters out times\n",
    "    text = re.sub(r'\\d+',\" \",text) #filters out numbers\n",
    "    text = re.sub(r\"[^A-Za-z0-9 ]+\",\" \",text) # filters out punctuation\n",
    "    text = re.sub(r\"XXXXX\",\" \",text) #filters out anonymised user\n",
    "    text = re.sub(r\"HHHHH\",\" \",text)\n",
    "    text = re.sub(r\"sshd\",\" \",text) #sshd comes up in every log and is not required\n",
    "    text = re.sub(r\"ruser\",\"user\",text)\n",
    "    text = re.sub(r\"rhost\",\"host\",text)\n",
    "    text = re.sub(r\"euid\",\"user\",text)\n",
    "    text = text.lower()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate exactly what the above does on a basic example we define this text file (featuring fake snippets of similar form to those in the log files) and output to check what kind of an effect our function will have on the strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bill said  hello  how are you doing           user host              password failed for   on port     ssh tty  uid '"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Bill said: hello, how are you doing?   112.116.2.45   114.117.45.234  ruser rhost 25.6.7   01:25:53  17:59:23 password failed for XXXXX on port: 73  ssh=tty  uid=\"\n",
    "regex(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenising\n",
    "\n",
    "Now we define the second of the two main functions we need for text cleaning. In the below function we define stop words, and remove them from the data, as well as lemmatising words that could be similar but have different endings (e.g. failure and fail). We also tokenise the data so that for each data point we have a list of words that will be used in our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_stop = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "def get_lemma(word):\n",
    "    return WordNetLemmatizer().lemmatize(word)\n",
    "\n",
    "def tokenizer(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [token for token in tokens if len(token) > 3] #filter out words that are shorter than 4 characters\n",
    "    tokens = [token for token in tokens if token not in en_stop] #filter out stop words\n",
    "    tokens = [get_lemma(token) for token in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function that is a combination of both above functions to apply to the text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    complete = tokenizer(regex(text))\n",
    "    return complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check the effect this function has on our strings by testing on a few randomly selected inputs from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original document: \n",
      "Jan  5 08:16:28 161.166.1.23 sshd[31213]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=216.186.0.161  user=XXXXX\n",
      "\n",
      " tokenized data: \n",
      "['08:16:28', '161.166.1.23', 'sshd', '31213', 'pam_unix', 'sshd', 'auth', 'authentication', 'failure', 'logname=', 'uid=0', 'euid=0', 'tty=ssh', 'ruser=', 'rhost=216.186.0.161', 'user=XXXXX']\n",
      "\n",
      " fully processed data: \n",
      "['unix', 'auth', 'authentication', 'failure', 'logname', 'user', 'user', 'host', 'user']\n"
     ]
    }
   ],
   "source": [
    "from gensim import parsing\n",
    "doc_sample = data_text[data_text['index'] == 16].values[0][0]\n",
    "\n",
    "print(\"original document: \")\n",
    "print(doc_sample)\n",
    "print(\"\\n tokenized data: \")\n",
    "print(tokenizer(doc_sample))\n",
    "print(\"\\n fully processed data: \")\n",
    "print(preprocess(doc_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original document: \n",
      "Jan  5 08:16:48 161.166.1.23 sshd[31217]: Failed password for XXXXX from 216.186.0.161 port 2506 ssh2\n",
      "\n",
      " tokenized data: \n",
      "['08:16:48', '161.166.1.23', 'sshd', '31217', 'Failed', 'password', 'XXXXX', '216.186.0.161', 'port', '2506', 'ssh2']\n",
      "\n",
      " fully processed data: \n",
      "['failed', 'password', 'port']\n"
     ]
    }
   ],
   "source": [
    "doc_sample = data_text[data_text['index'] == 18].values[0][0]\n",
    "\n",
    "print(\"original document: \")\n",
    "print(doc_sample)\n",
    "print(\"\\n tokenized data: \")\n",
    "print(tokenizer(doc_sample))\n",
    "print(\"\\n fully processed data: \")\n",
    "print(preprocess(doc_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original document: \n",
      "Feb 18 15:04:46 161.166.5.240 sshd[21299]: error: PAM: Authentication failure for XXXXX from 82.122.154.220\n",
      "\n",
      " tokenized data: \n",
      "['15:04:46', '161.166.5.240', 'sshd', '21299', 'error', 'Authentication', 'failure', 'XXXXX', '82.122.154.220']\n",
      "\n",
      " fully processed data: \n",
      "['error', 'authentication', 'failure']\n"
     ]
    }
   ],
   "source": [
    "doc_sample = data_text[data_text['index'] == 270564].values[0][0]\n",
    "\n",
    "print(\"original document: \")\n",
    "print(doc_sample)\n",
    "print(\"\\n tokenized data: \")\n",
    "print(tokenizer(doc_sample))\n",
    "print(\"\\n fully processed data: \")\n",
    "print(preprocess(doc_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "testvar = preprocess(doc_sample)\n",
    "print(type(testvar))\n",
    "print(type(testvar[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we quickly test and compare what happens if we apply this function to the first 20 values in our dataset, and find the results that we expect to see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs = data_text['log'][:20].map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Jan  5 08:16:48 161.166.1.23 sshd[31217]: Failed password for XXXXX from 216.186.0.161 port 2506 ssh2'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_text[\"log\"][18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['failed', 'password', 'port']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs[18]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to conduct the filtering on the whole dataset, first of all checking the length, just to make sure we have the same length data after processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1588052"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed = data_text['log'].map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                   [failed, password, port]\n",
       "1             [disconnecting, many, authentication, failure]\n",
       "2          [unix, auth, authentication, failure, logname,...\n",
       "3                               [service, ignoring, retries]\n",
       "4          [unix, auth, authentication, failure, logname,...\n",
       "                                 ...                        \n",
       "1588047                  [user, allowed, listed, allowusers]\n",
       "1588048                                      [invalid, user]\n",
       "1588049                                      [invalid, user]\n",
       "1588050                                      [invalid, user]\n",
       "1588051                                      [invalid, user]\n",
       "Name: log, Length: 1588052, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the snippets we can see in the above data it seems that our processing has been quite successful, reducing and cleaning the text data considerably."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus and Dictionary creation\n",
    "\n",
    "We are going to generate a dictionary and corpus for use in topic models. We are generating them here in this document and will then write these to Python pickle files so that we do not have to repeatedly compute them and also so that each model is using the same corpus so that our comparisons are more accurate. We start by generating a dictionary from our processed data above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 failed\n",
      "1 password\n",
      "2 port\n",
      "3 authentication\n",
      "4 disconnecting\n",
      "5 failure\n",
      "6 many\n",
      "7 auth\n",
      "8 host\n",
      "9 logname\n",
      "10 unix\n",
      "613\n"
     ]
    }
   ],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed)\n",
    "\n",
    "count = 0\n",
    "for k,v  in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break\n",
    "print(len(dictionary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important to filter out extremes in dictionaries, here we filter out any terms that come up a disproportionately small or large. We see that this has cut down the number of junk terms considerably reducing the length of the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "350"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary2 = dictionary\n",
    "dictionary2.filter_extremes(no_below=10, no_above=0.5, keep_n=100000)\n",
    "len(dictionary2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now write the pickle, the below script first checks a pickle already exists in the correct folder of the GitHub repository and if not proceeds to generate the corpus pickle and dumps both the corpus and dictionary pickles to the relevant folder on my computer that corresponds to the GitHubDesktop repository; we will then push this data when generated to be able to be accessed from anywhere with an internet connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to read corpus from pickle\n",
      "Pickle not found, creating corpus and saving to pickle\n",
      "Pickle saved. Time taken: 0:00:17.875929\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print(\"Attempting to read corpus from pickle\")\n",
    "    fp=gzip.open(urlopen('https://github.com/Galeforse/DST-Assessment-03/raw/master/Data/main/corpus_10.pkl.gz'),'rb')\n",
    "    corpus=pickle.load(fp)\n",
    "    fp.close()\n",
    "    print(\"Corpus read from pickle.\")\n",
    "except HTTPError as err:\n",
    "    if err.code == 404:\n",
    "        print(\"Pickle not found, creating corpus and saving to pickle\")\n",
    "        start=dt.datetime.now()\n",
    "        corpus = [dictionary2.doc2bow(doc) for doc in processed]\n",
    "        fp=gzip.open('G:/Users/Gabriel/Documents/Education/UoB/GitHubDesktop/DST-Assessment-03/data/main/corpus_10.pkl.gz','wb')\n",
    "        pickle.dump(corpus,fp)\n",
    "        fp.close()\n",
    "        pickle.dump(corpus, open('G:/Users/Gabriel/Documents/Education/UoB/GitHubDesktop/DST-Assessment-03/data/main/corpus_10.pkl', 'wb'))\n",
    "        pickle.dump(dictionary2, open('G:/Users/Gabriel/Documents/Education/UoB/GitHubDesktop/DST-Assessment-03/data/main/dictionary_10.pkl', 'wb'))\n",
    "        print(\"Pickle saved. Time taken: \" + str(dt.datetime.now()-start))\n",
    "    else:\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now these pickles have been generated they can be imported for use in our models that follow in the rest of the report, without the need for each file and user to apply the data processing themselves (as can be computationally expensive)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
